<!doctype html><html lang="en" class="no-js"><head><meta charset="utf-8"> <!-- begin SEO --><title>Blog - Birken Silitch (이지상)</title><meta name="description" content="Birken Silitch"><link rel="canonical" href="https://dlwltkd.github.io//blog/"> <script type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : " Birken Silitch (이지상) ", "url" : "https://dlwltkd.github.io/", "sameAs" : null } </script> <!-- end SEO --> <!-- Open Graph protocol data (https://ogp.me/), used by social media --><meta property="og:locale" content="en-US"><meta property="og:site_name" content="Birken Silitch (이지상)"><meta property="og:title" content="Blog"><meta property="og:url" content="https://dlwltkd.github.io//blog/"> <!-- end Open Graph protocol --><link href="https://dlwltkd.github.io//feed.xml" type="application/atom+xml" rel="alternate" title="Birken Silitch (이지상) Feed"> <!-- http://t.co/dKP3o1e --><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width, initial-scale=1.0"> <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script> <!-- For all browsers --><link rel="stylesheet" href="https://dlwltkd.github.io//assets/css/main.css"><meta http-equiv="cleartype" content="on"> <!-- start custom head snippets --> <!-- Support for Academicons --><link rel="stylesheet" href="https://dlwltkd.github.io//assets/css/academicons.css"/> <!-- favicon from https://commons.wikimedia.org/wiki/File:OOjs_UI_icon_academic-progressive.svg --><link rel="apple-touch-icon" sizes="180x180" href="https://dlwltkd.github.io//images/apple-touch-icon-180x180.png"/><link rel="icon" type="image/svg+xml" href="https://dlwltkd.github.io//images/favicon.svg"/><link rel="icon" type="image/png" href="https://dlwltkd.github.io//images/favicon-32x32.png" sizes="32x32"/><link rel="icon" type="image/png" href="https://dlwltkd.github.io//images/favicon-192x192.png" sizes="192x192"/><link rel="manifest" href="https://dlwltkd.github.io//images/manifest.json"/><link rel="icon" href="/images/favicon.ico"/><meta name="theme-color" content="#ffffff"/> <!-- end custom head snippets --></head><body> <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]--><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav"> <button><div class="navicon"></div></button><ul class="visible-links"><li class="masthead__menu-item masthead__menu-item--lg persist"><a href="https://dlwltkd.github.io//">Birken Silitch (이지상)</a></li><li class="masthead__menu-item"><a href="https://dlwltkd.github.io//files/Academic_CV_BirkenSilitch.pdf">CV</a></li><li class="masthead__menu-item"><a href="https://dlwltkd.github.io//blog/">Blog</a></li><li id="theme-toggle" class="masthead__menu-item persist tail"> <a role="button" aria-labelledby="theme-icon"><i id="theme-icon" class="fa-solid fa-sun" aria-hidden="true" title="toggle theme"></i></a></li></ul><ul class="hidden-links hidden"></ul></nav></div></div></div><div id="main" role="main"><div class="sidebar sticky"><div itemscope itemtype="http://schema.org/Person"><div class="author__avatar"> <img src="https://dlwltkd.github.io//images/profile1.png" class="author__avatar" alt="Birken Silitch (이지상) " fetchpriority="high" /></div><div class="author__content"><h3 class="author__name">Birken Silitch (이지상)</h3><p class="author__bio">AI Student at Yonsei University</p></div><div class="author__urls-wrapper"> <button class="btn btn--inverse">Follow</button><ul class="author__urls social-icons"> <!-- Font Awesome icons / Biographic information --><li class="author__desktop"><i class="fas fa-fw fa-location-dot icon-pad-right" aria-hidden="true"></i>Seoul, Korea</li><li class="author__desktop"><i class="fas fa-fw fa-building-columns icon-pad-right" aria-hidden="true"></i>Yonsei University</li><li><a href="mailto:dlwltkd@yonsei.ac.kr"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>Email</a></li><!-- Font Awesome and Academicons icons / Academic websites --> <!-- Font Awesome icons / Repositories and software development --><li><a href="https://github.com/dlwltkd"><i class="fab fa-fw fa-github icon-pad-right" aria-hidden="true"></i>GitHub</a></li><!-- Font Awesome icons / Social media --><li><a href="https://www.linkedin.com/in/birken-silitch-a1414a1b0"><i class="fab fa-fw fa-linkedin icon-pad-right" aria-hidden="true"></i>LinkedIn</a></li></ul></div></div></div><article class="page" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="headline" content="Blog"><div class="page__inner-wrap"><header><h1 class="page__title" itemprop="headline"><strong>Blog</strong></h1></header><section class="page__content" itemprop="text"><hr /><p>layout: post title: “VQ-VAE: How Neural Networks Learned to Speak in Symbols” subtitle: “An illustrated, math-backed walkthrough of Neural Discrete Representation Learning” tags: [deep learning, vq-vae, generative models, representation learning] —</p><blockquote><p>Based on “Neural Discrete Representation Learning” (van den Oord, Vinyals, &amp; Kavukcuoglu, NeurIPS 2017).</p></blockquote><p><img src="/assets/vq-vae/codebook-grid.png" alt="Codebook sketch placeholder" /> <em>Figure: The encoder output snaps to the nearest code “cell” in a learned vocabulary (codebook).</em></p><h2 id="why-this-paper-still-matters">🌟 Why this paper still matters</h2><p>Imagine if a neural net could invent its <strong>own vocabulary</strong> for perception. That’s the idea behind <strong>VQ-VAE</strong>: learn <strong>discrete</strong> latent tokens (like words) that represent high-level structure in images, speech, and video—<strong>without labels</strong>. This simple twist avoids the “posterior collapse” that plagues VAEs and paved the way for models like <strong>VQ-VAE-2</strong>, <strong>DALL·E</strong>, <strong>Jukebox</strong>, and modern neural codecs.</p><p><strong>Prefer watching first?</strong></p><iframe width="560" height="315" src="https://www.youtube.com/embed/VZFVUrYcig0" title="VQ-VAE Explained" frameborder="0" allowfullscreen=""></iframe><hr /><h2 id="background-in-60-seconds">🧱 Background in 60 seconds</h2><p><strong>Autoencoder (AE):</strong> compress \(x \to z\), reconstruct \(\hat{x}\).<br /> Loss: \(L_{\text{AE}}=|x-\hat{x}|^2\).</p><p><strong>Variational Autoencoder (VAE):</strong> learns a distribution \(q(z|x)\), samples \(z\), reconstructs: \[ L_{\text{VAE}}=\mathbb{E}_{q(z|x)}[-\log p(x|z)] + \mathrm{KL}\big(q(z|x)|p(z)\big). \]</p><p><strong>Problem — Posterior collapse:</strong> powerful decoders (e.g., PixelCNN/LSTM) can ignore \(z\).<br /> We want a latent that the decoder <strong>must</strong> use, and ideally it should be <strong>discrete</strong> (closer to words/phonemes).</p><hr /><h2 id="the-vq-vae-idea">💡 The VQ-VAE idea</h2><p>Replace continuous \(z\) with <strong>indices into a learned codebook</strong> \(e={e_1,\dots,e_K}\).</p><p><strong>Pipeline</strong> 1) <strong>Encoder:</strong> \(x \mapsto z_e(x)\) (continuous)<br /> 2) <strong>Quantize:</strong> choose nearest codeword<br /> \[ k=\arg\min_j |z_e(x)-e_j|^2,\quad z_q(x)=e_k \] 3) <strong>Decoder:</strong> reconstruct from \(z_q(x)\)</p><p><strong>Training objective</strong> \[ \begin{aligned} L &amp;= \underbrace{|x-\hat{x}|^2}<em>{\text{reconstruction}} <br /> &amp;\quad + \underbrace{|\mathrm{sg}[z_e(x)]-e|_2^2}</em>{\text{codebook update}} <br /> &amp;\quad + \beta\,\underbrace{|z_e(x)-\mathrm{sg}[e]|<em>2^2}</em>{\text{commitment}} \end{aligned} \] <code class="language-plaintext highlighter-rouge">sg[·]</code> is <em>stop-gradient</em>.</p><p><strong>Why it works</strong> - Discrete lookup forces the decoder to condition on tokens → no collapse.<br /> - Straight-through gradient to the encoder keeps training stable.<br /> - The codebook becomes a <strong>vocabulary of concepts</strong>.</p><hr /><h2 id="what-the-paper-shows">🧪 What the paper shows</h2><h3 id="images-imagenet-128128">🖼 Images (ImageNet 128×128)</h3><ul><li>Latent grid: \(32\times32\times1\), codebook size \(K=512\) → ~40× compression.</li><li>Reconstructions are coherent; slight blur only.</li><li>Train a <strong>PixelCNN prior</strong> over the discrete codes and decode samples → realistic images (foxes, whales, reefs).</li></ul><p><img src="/assets/vq-vae/reconstruction-triptych.png" alt="Reconstruction placeholder" /> <em>Original | Reconstruction | Sample from PixelCNN prior</em></p><h3 id="speech-vctk-109-speakers">🔊 Speech (VCTK, 109 speakers)</h3><ul><li>Encoder compresses waveform ~64×; tokens capture <strong>content</strong> (what is said), not speaker identity.</li><li><strong>Speaker conversion:</strong> same content, change the speaker id at the decoder → same sentence, new voice.</li><li>Discovered <strong>phoneme-like</strong> clusters without labels (~49% phoneme accuracy vs ~7% random).</li></ul><blockquote><p>Demos: <a href="https://avdnoord.github.io/homepage/vqvae/">https://avdnoord.github.io/homepage/vqvae/</a></p></blockquote><h3 id="video-deepmind-lab">🎮 Video (DeepMind Lab)</h3><ul><li>Predict future frames <strong>in latent space</strong> conditioned on actions (e.g., “move forward”).</li><li>Decode afterward → crisp sequences with consistent geometry.</li></ul><hr /><h2 id="numbers-cifar-10-bitsdim-">📊 Numbers (CIFAR-10, bits/dim ↓)</h2><p>| Model | Latent | Bits/Dim | |—————-|————-|———-| | Continuous VAE | continuous | 4.51 | | <strong>VQ-VAE</strong> | <strong>discrete</strong>| <strong>4.67</strong> | | VIMCO | discrete | 5.14 |</p><p>VQ-VAE is the <strong>first discrete-latent VAE</strong> to approach continuous VAE likelihoods—while giving you a symbolic code.</p><hr /><h2 id="why-it-mattered-and-still-does">🧭 Why it mattered (and still does)</h2><ul><li>Made <strong>discrete latents</strong> easy to train → tokens for images/audio.</li><li>Scales beautifully with strong priors (PixelCNN, Transformers).</li><li>Influenced <strong>VQ-VAE-2</strong>, <strong>DALL·E</strong>, <strong>Jukebox</strong>, <strong>EnCodec</strong> and many modern tokenizers.</li></ul><p><strong>Big picture:</strong> VQ-VAE bridges continuous perception and discrete reasoning.</p><hr /><h2 id="closing-thought">🧠 Closing thought</h2><p>Humans think in <strong>discrete</strong> concepts: “edge,” “phoneme,” “chair.”<br /> VQ-VAE lets neural nets <strong>learn such concepts</strong> automatically—and then <em>speak</em> in them.</p><p>Whenever you see AI synthesizing images or music from tokens, you’re hearing echoes of this paper.</p><hr /><h2 id="references--links">📚 References &amp; links</h2><ul><li>van den Oord, Vinyals, Kavukcuoglu. <em>Neural Discrete Representation Learning.</em> NeurIPS 2017.</li><li>Yannic Kilcher: <em>VQ-VAE Explained</em> (YouTube).</li><li>Follow-ups: VQ-VAE-2 (2019), DALL·E (2021), Jukebox (2020), EnCodec (2022).</li></ul></section><footer class="page__meta"></footer></div></article></div><div class="page__footer"><footer> <!-- MathJax --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] } }; </script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script> <!-- Support for Plotly --> <script defer src='https://cdnjs.cloudflare.com/ajax/libs/plotly.js/3.0.1/plotly.min.js'></script> <!-- Support for Mermaid --> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs'; mermaid.initialize({startOnLoad:true, theme:'default'}); await mermaid.run({querySelector:'code.language-mermaid'}); </script> <!-- end custom footer snippets --><div class="page__footer-follow"><ul class="social-icons"><li><strong>Follow:</strong></li><li> <a href="https://github.com/dlwltkd"> <i class="fab fa-github" aria-hidden="true"></i> GitHub </a></li></ul></div><div class="page__footer-copyright"> &copy; 2025 Birken Silitch (이지상) , Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.<br /> Site last updated 2025-10-26</div></footer></div><script type="module" src="https://dlwltkd.github.io//assets/js/main.min.js"></script></body></html>

